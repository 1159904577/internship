## 2024.12.10   学习Flink

理论：学习Flink理论知识：Flink连接操作、各种窗口类型、流数据去重、Slot/并行度、异步操作（完结）

实践：安装Flink，查看Flink的Web ui界面，在idea中开发Flink程序WordCount，在Flink管理页面打包上传

#### Flink连接操作

| **连接类型** | **描述**                                             | **使用场景**                                     | **时间要求**                   |      |
| ------------ | ---------------------------------------------------- | ------------------------------------------------ | ------------------------------ | ---- |
| **窗口连接** | 基于时间窗口的连接，两个流中的数据仅在时间窗口内连接 | 适用于时间敏感的流数据连接（如订单与支付）       | 需要指定窗口大小（如 10 秒）   |      |
| **间隔连接** | 基于时间间隔的连接，允许事件之间有一定的时间偏差     | 用于时间间隔内的事件匹配，如支付和订单之间的间隔 | 需要指定时间范围（如 ±5 秒）   |      |
| **内部连接** | 基于键值的匹配，只有匹配的事件才会连接               | 适用于只关心数据完全匹配的场景                   | 没有时间要求，只依赖于键值匹配 |      |

##### 窗口连接

```
//有两个流，orderStream 和 paymentStream，在 10 秒的时间窗口内连接这两个流，确保在时间范围内的订单和支付可以正确匹配。
SingleOutputStreamOperator<JoinedOrderPayment> joinedStream = 
    orderStream
    .keyBy(Order::getOrderId)  // 按订单ID分区
    .window(TumblingEventTimeWindows.of(Time.seconds(10)))  // 10秒滚动窗口
    .join(paymentStream.keyBy(Payment::getOrderId))  // 按订单ID连接
    .where(order -> order.getOrderId())
    .equalTo(payment -> payment.getOrderId())
    .apply(new JoinFunction());  // 自定义连接函数

```

##### 间隔连接

```
//在订单和支付之间建立一个间隔连接，要求支付事件与订单事件的时间差不超过 5 秒
SingleOutputStreamOperator<JoinedOrderPayment> intervalJoinedStream = 
    orderStream
    .keyBy(Order::getOrderId)  // 按订单ID分区
    .intervalJoin(paymentStream.keyBy(Payment::getOrderId))  // 基于订单ID进行间隔连接
    .between(Time.seconds(-5), Time.seconds(5))  // 时间间隔为±5秒
    .process(new JoinFunction());  // 自定义连接函数

```

##### 内连接

```
//有两个流，orderStream 和 paymentStream，我们希望通过订单 ID 进行内连接，只有当订单和支付有匹配的 ID 时才输出结果
SingleOutputStreamOperator<JoinedOrderPayment> innerJoinedStream = 
    orderStream
    .keyBy(Order::getOrderId)  // 按订单ID分区
    .join(paymentStream.keyBy(Payment::getOrderId))  // 通过订单ID进行内连接
    .where(order -> order.getOrderId())
    .equalTo(payment -> payment.getOrderId())
    .apply(new JoinFunction());  // 自定义连接函数

```

##### 补充

**流与表连接**

先将流转为表，再通过表连接，最后转化为流

```
StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

// 将流转换为表
Table orderTable = tableEnv.fromDataStream(orderStream, Schema.newBuilder()
    .column("orderId", DataTypes.STRING())
    .column("customerId", DataTypes.STRING())
    .column("amount", DataTypes.DECIMAL())
    .build());

// 从表中查询客户信息
Table customerTable = tableEnv.sqlQuery("SELECT customerId, name FROM Customers");

Table result = tableEnv.sqlQuery(
    "SELECT o.orderId, o.amount, c.name " +
    "FROM " + orderTable + " AS o " +
    "JOIN " + customerTable + " AS c " +
    "ON o.customerId = c.customerId"
);

// 将结果转换回流
DataStream<Row> resultStream = tableEnv.toDataStream(result);

```

**非对称连接**

连接一个小流和一个大流来执行 Join 操作

**主流**：通常是较小的数据流，可以完全或部分地保存在内存中进行连接处理。

**辅流**：通常是较大的数据流，通常是一个不断变化的流。

**工作原理：**

1. **主流（主数据流）**会被广播到所有的操作符。
2. **辅流（大数据流）**会继续在流式数据环境中处理，并与广播的主流进行连接。
3. 每个操作符节点收到广播流之后，会将其缓存，并将其与进入该节点的辅流进行 Join 操作。

```
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// 主流：用户信息流（较小）
DataStream<UserInfo> userStream = env.fromElements(
    new UserInfo("user1", "Alice"),
    new UserInfo("user2", "Bob"),
    new UserInfo("user3", "Charlie")
);

// 辅流：交易记录流（较大）
DataStream<Transaction> transactionStream = env.fromElements(
    new Transaction("user1", 100),
    new Transaction("user2", 200),
    new Transaction("user1", 150),
    new Transaction("user3", 250)
);

// 主流广播
BroadcastStream<UserInfo> broadcastStream = userStream.broadcast(UserInfoBroadcastStateDescriptor);

// 执行广播 Join 操作
DataStream<TransactionWithUserInfo> joinedStream = transactionStream
    .connect(broadcastStream)
    .process(new BroadcastJoinFunction());

// 定义 Broadcast State
public static class UserInfoBroadcastStateDescriptor extends MapStateDescriptor<String, UserInfo> {
    public UserInfoBroadcastStateDescriptor() {
        super("userInfo", String.class, UserInfo.class);
    }
}

public static class BroadcastJoinFunction extends KeyedBroadcastProcessFunction<String, Transaction, UserInfo, TransactionWithUserInfo> {
    private MapState<String, UserInfo> userInfoState;

    @Override
    public void open(Configuration parameters) throws Exception {
        userInfoState = getRuntimeContext().getMapState(new UserInfoBroadcastStateDescriptor());
    }

    @Override
    public void processElement(Transaction transaction, ReadOnlyContext ctx, Collector<TransactionWithUserInfo> out) throws Exception {
        UserInfo userInfo = userInfoState.get(transaction.getUserId());
        if (userInfo != null) {
            out.collect(new TransactionWithUserInfo(transaction, userInfo));
        }
    }

    @Override
    public void processBroadcastElement(UserInfo userInfo, Context ctx, Collector<TransactionWithUserInfo> out) throws Exception {
        userInfoState.put(userInfo.getUserId(), userInfo);
    }
}

public static class TransactionWithUserInfo {
    private Transaction transaction;
    private UserInfo userInfo;

    public TransactionWithUserInfo(Transaction transaction, UserInfo userInfo) {
        this.transaction = transaction;
        this.userInfo = userInfo;
    }
}

```

#### 窗口类型

##### 滚动窗口

```
//每 10 秒对订单数据进行汇总
SingleOutputStreamOperator<AggregatedOrder> resultStream = 
    orderStream
    .keyBy(Order::getOrderId)  // 按订单ID分区
    .window(TumblingEventTimeWindows.of(Time.seconds(10)))  // 每10秒一个滚动窗口
    .apply(new WindowFunction<>());
```

##### 滑动窗口

```
//对过去 10 秒内的数据每 5 秒进行一次聚合，也就是说窗口每 5 秒滑动一次
SingleOutputStreamOperator<AggregatedOrder> resultStream = 
    orderStream
    .keyBy(Order::getOrderId)  // 按订单ID分区
    .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5)))  // 窗口大小为10秒，滑动步长为5秒
    .apply(new WindowFunction<>());
```

窗口会按以下时间段进行划分：

- 第一个窗口：0-10 秒
- 第二个窗口：5-15 秒
- 第三个窗口：10-20 秒

##### 会话窗口

```
//有一个用户访问流，当用户连续访问的时间间隔小于 10 分钟时，认为是同一个会话
SingleOutputStreamOperator<SessionAggregation> resultStream = 
    userStream
    .keyBy(User::getUserId)  // 按用户ID分区
    .window(EventTimeSessionWindows.withGap(Time.minutes(10)))  // 会话窗口，间隔小于10分钟
    .apply(new SessionWindowFunction<>());
```

#### 流数据去重处理

```
//基于KeyedState去重
//有一个流 orderStream，每个事件都包含一个 orderId，我们希望去重订单 ID，确保每个订单只处理一次
public class DeduplicateOrders extends KeyedProcessFunction<String, Order, Order> {

    // 用来保存每个订单ID是否已经出现过
    private ValueState<Boolean> orderState;

    @Override
    public void open(Configuration parameters) {
        // 创建一个值状态来存储订单ID的去重信息
        ValueStateDescriptor<Boolean> descriptor = new ValueStateDescriptor<>(
                "order-state", // 状态的名称
                Boolean.class, // 状态的类型
                false // 默认值
        );
        orderState = getRuntimeContext().getState(descriptor);
    }

    @Override
    public void processElement(Order order, Context context, Collector<Order> collector) throws Exception {
        // 如果订单ID已经出现过，忽略该事件
        if (!orderState.value()) {
            collector.collect(order);  // 如果是新订单，则输出
            orderState.update(true);    // 更新状态，表示该订单已经处理过
        }
    }
}
```

#### Slot和并行度

Slot：Flink作业运行时资源的基本单位，TaskManager包含一定数量的Slot，Flink使用Slot来分配作业的并行任务

Slot共享：允许多个操作符共享一个Slot。例如：一个 TaskManager有4个slot,如果每个slot 只能运行一个任务，那么除非有4个不同的任务并行运行，否则总会有未使用的资源。而通过 Taskslot共享，我们可以让多个任务共享同一个 Slot，前提是这些任务属于同一个作业或是定义在统一资源组里的。这样就可以最大限度地使用可用资源，提高计算效率。

并行度：每个操作符（Operator）在任务执行时的并发度，也就是每个操作符会分配多少个并行任务实例

例：有一个 Flink 集群，配置了 2 个 TaskManager，每个 TaskManager 上有 4 个 Slot。然后我们设置了一个作业的并行度为 8。Flink 会将作业的任务分配到 2 个 TaskManager 上，每个 TaskManager 分配 4 个 Slot 来执行 8 个并行任务。

#### 异步操作

##### 为什么要使用异步操作

在大数据处理中，经常需要访问外部系统来获取补充数据或者执行某些操作。例如：查询数据库获取详细信息或调用外部API进行实时数据验证，这些操作都是IO密集型的，容易成为系统瓶颈。如果采用同步方式，任务需要等待外部系统返回结果，容易造成大量计算资源浪费。而采用异步方式，可以在等待响应的同时处理任务，从而提高资源利用率和系统吞吐量。

##### Flink异步操作实现

- 实现一个类继承RichAsyncFuncion<IN,OUT>
- 在该类中，实现asyncInvoke方法，该方法在异步操作时被调用
- 使用Flink的AsyncDataStream来包装异步函数并进行数据流处理

```
import org.apache.flink.streaming.api.functions.async.AsyncFunction;
import org.apache.flink.streaming.api.functions.async.ResultFuture;
import org.apache.flink.streaming.api.datastream.AsyncDataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.datastream.DataStream;
import java.util.concurrent.TimeUnit;
import java.util.Arrays;

public class AsyncIOExample {

    public static class MyAsyncFunction implements AsyncFunction<Integer, String> {

        @Override
        public void asyncInvoke(Integer input, ResultFuture<String> resultFuture) {
            // 模拟异步调用外部系统
            new Thread(() -> {
                try {
                    // 模拟异步操作耗时
                    Thread.sleep(100);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                // 返回结果
                resultFuture.complete(Arrays.asList("Result for " + input));
            }).start();
        }
    }

    public static void main(String[] args) throws Exception {
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<Integer> inputStream = env.fromElements(1, 2, 3, 4, 5);

        DataStream<String> asyncResultStream = AsyncDataStream
                .unorderedWait(inputStream, new MyAsyncFunction(), 1000, TimeUnit.MILLISECONDS);

        asyncResultStream.print();

        env.execute("Async IO Example");
    }
}
```

### Flink镜像站

https://mirrors.aliyun.com/apache/flink/

### 问题

Q:新版Flink没有bat启动文件

A:在bin目录下新建flink.bat和start-cluster.bat文件。内容如下：

flink.bat

```
::###############################################################################
::  Licensed to the Apache Software Foundation (ASF) under one
::  or more contributor license agreements.  See the NOTICE file
::  distributed with this work for additional information
::  regarding copyright ownership.  The ASF licenses this file
::  to you under the Apache License, Version 2.0 (the
::  "License"); you may not use this file except in compliance
::  with the License.  You may obtain a copy of the License at
::
::      http://www.apache.org/licenses/LICENSE-2.0
::
::  Unless required by applicable law or agreed to in writing, software
::  distributed under the License is distributed on an "AS IS" BASIS,
::  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
::  See the License for the specific language governing permissions and
:: limitations under the License.
::###############################################################################
 
@echo off
setlocal
 
SET bin=%~dp0
SET FLINK_HOME=%bin%..
SET FLINK_LIB_DIR=%FLINK_HOME%\lib
SET FLINK_PLUGINS_DIR=%FLINK_HOME%\plugins
 
SET JVM_ARGS=-Xmx512m
 
SET FLINK_JM_CLASSPATH=%FLINK_LIB_DIR%\*
 
java %JVM_ARGS% -cp "%FLINK_JM_CLASSPATH%"; org.apache.flink.client.cli.CliFrontend %*
 
endlocal
```

start-cluster.bat

```
::###############################################################################
::  Licensed to the Apache Software Foundation (ASF) under one
::  or more contributor license agreements.  See the NOTICE file
::  distributed with this work for additional information
::  regarding copyright ownership.  The ASF licenses this file
::  to you under the Apache License, Version 2.0 (the
::  "License"); you may not use this file except in compliance
::  with the License.  You may obtain a copy of the License at
::
::      http://www.apache.org/licenses/LICENSE-2.0
::
::  Unless required by applicable law or agreed to in writing, software
::  distributed under the License is distributed on an "AS IS" BASIS,
::  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
::  See the License for the specific language governing permissions and
:: limitations under the License.
::###############################################################################
 
@echo off
setlocal EnableDelayedExpansion
 
SET bin=%~dp0
SET FLINK_HOME=%bin%..
SET FLINK_LIB_DIR=%FLINK_HOME%\lib
SET FLINK_PLUGINS_DIR=%FLINK_HOME%\plugins
SET FLINK_CONF_DIR=%FLINK_HOME%\conf
SET FLINK_LOG_DIR=%FLINK_HOME%\log
 
SET JVM_ARGS=-Xms1024m -Xmx1024m
 
SET FLINK_CLASSPATH=%FLINK_LIB_DIR%\*
 
SET logname_jm=flink-%username%-jobmanager.log
SET logname_tm=flink-%username%-taskmanager.log
SET log_jm=%FLINK_LOG_DIR%\%logname_jm%
SET log_tm=%FLINK_LOG_DIR%\%logname_tm%
SET outname_jm=flink-%username%-jobmanager.out
SET outname_tm=flink-%username%-taskmanager.out
SET out_jm=%FLINK_LOG_DIR%\%outname_jm%
SET out_tm=%FLINK_LOG_DIR%\%outname_tm%
 
SET log_setting_jm=-Dlog.file="%log_jm%" -Dlogback.configurationFile=file:"%FLINK_CONF_DIR%/logback.xml" -Dlog4j.configuration=file:"%FLINK_CONF_DIR%/log4j.properties"
SET log_setting_tm=-Dlog.file="%log_tm%" -Dlogback.configurationFile=file:"%FLINK_CONF_DIR%/logback.xml" -Dlog4j.configuration=file:"%FLINK_CONF_DIR%/log4j.properties"
 
:: Log rotation (quick and dirty)
CD "%FLINK_LOG_DIR%"
for /l %%x in (5, -1, 1) do (
SET /A y = %%x+1
RENAME "%logname_jm%.%%x" "%logname_jm%.!y!" 2> nul
RENAME "%logname_tm%.%%x" "%logname_tm%.!y!" 2> nul
RENAME "%outname_jm%.%%x" "%outname_jm%.!y!"  2> nul
RENAME "%outname_tm%.%%x" "%outname_tm%.!y!"  2> nul
)
RENAME "%logname_jm%" "%logname_jm%.0"  2> nul
RENAME "%logname_tm%" "%logname_tm%.0"  2> nul
RENAME "%outname_jm%" "%outname_jm%.0"  2> nul
RENAME "%outname_tm%" "%outname_tm%.0"  2> nul
DEL "%logname_jm%.6"  2> nul
DEL "%logname_tm%.6"  2> nul
DEL "%outname_jm%.6"  2> nul
DEL "%outname_tm%.6"  2> nul
 
for %%X in (java.exe) do (set FOUND=%%~$PATH:X)
if not defined FOUND (
    echo java.exe was not found in PATH variable
    goto :eof
)
 
echo Starting a local cluster with one JobManager process and one TaskManager process.
 
echo You can terminate the processes via CTRL-C in the spawned shell windows.
 
echo Web interface by default on http://localhost:8081/.
 
start java %JVM_ARGS% %log_setting_jm% -cp "%FLINK_CLASSPATH%"; org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint --configDir "%FLINK_CONF_DIR%" > "%out_jm%" 2>&1
start java %JVM_ARGS% %log_setting_tm% -cp "%FLINK_CLASSPATH%"; org.apache.flink.runtime.taskexecutor.TaskManagerRunner --configDir "%FLINK_CONF_DIR%" > "%out_tm%" 2>&1
 
endlocal
```

