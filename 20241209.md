## 2024.12.09   学习Flink

理论：学习Flink理论知识

实践：通过Flink批处理和流处理两种方式实现wordCount

#### 核心组件

JobManager：类比hdfs中namenode

TaskManager：类比hdfs中datanode

Job：编写的Flink应用程序

ExecutionGraph：将Job转换为多个并行执行的子任务

#### 流处理、批处理

##### 批处理

```
package org.smartloli.game.x.m.flink;

import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.operators.*;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.util.Collector;

/**
 * 需求：使用批处理的方式进行单词的计算
 * 读取文件中的数据，对单词进行空格拆分，对每个单词进行计算，然后分组累加
 *
 * flink1.12版本之前才区分流和批，flink1.12版本后实现流批的统一
 * Flink批处理的数据抽象：DataSet
 * Flink流处理的数据抽象：DataStream
 */
public class BatchWordCount {
    public static void main(String[] args) throws Exception {
        /**
         * 实现步骤：
         * 1）初始化flink批处理的运行环境
         * 2）指定文件路径，获取文件数据
         * 3）对获取到的数据进行空格拆分
         * 4）对拆分的单词进行计数，每个单词记一次数
         * 5）对相同的单词进行分组操作
         * 6）对分组后的数据进行累加操作
         * 7）打印输出（测试）
         * 8）启动作业，递交任务
         */
        //初始化flink批处理的运行环境（获取到当前环境，如果本地运行获取local环境）
        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

        //指定文件路径，获取文件数据
        final DataSource<String> lines = env.readTextFile("C:\\Users\\13719\\Documents\\game-x-m\\src\\main\\java\\org\\smartloli\\game\\x\\m\\flink\\test.txt");

        //对获取到的数据进行空格拆分
        //map与flatmap的区别
        //String：传入值类型
        //String：返回值类型
        /**
         * hello world
         * hello flink
         * hello scala
         * hello spark
         */
        final FlatMapOperator<String, String> words = lines.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public void flatMap(String line, Collector<String> out) throws Exception {
                //将每行字符串进行空格拆分
                final String[] dataArray = line.split(" ");
                //循环遍历字符串数组
                for (String word : dataArray) {
                    //需要使用out进行返回数据
                    out.collect(word);
                }
            }
        });

        /**
         * 对拆分的单词进行计数，每个单词记一次数
         * String：传入值类型
         * Tuple2<String, Integer>：返回值类型<单词, 单词次数>
         * 元祖对象最长可以传递25个参数，Tuple1 -> Tuple25， TupleN->N表示参数的个数
         */
        final MapOperator<String, Tuple2<String, Integer>> wordAndOne = words.map(new MapFunction<String, Tuple2<String, Integer>>() {
            @Override
            public Tuple2<String, Integer> map(String value) throws Exception {
                return Tuple2.of(value, 1);
            }
        });

        //对相同的单词进行分组操作
        final UnsortedGrouping<Tuple2<String, Integer>> grouped = wordAndOne.groupBy(0);

        //对分组后的数据进行累加操作
        final AggregateOperator<Tuple2<String, Integer>> summed = grouped.sum(1);

        //打印输出（测试）
        summed.print();

        //todo 8）启动作业，递交任务
        //在批处理开发中以下方法会触发作业的递交操作，故无需 env.execute()
        //'execute()', 'count()', 'collect()', or 'print()'.
        //env.execute();
    }
}


```

输出结果：

```
(flink,2)
(world,1)
(abc,1)
(hello,4)
```

##### 流处理

```
package org.smartloli.game.x.m.flink;

import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.java.tuple.Tuple;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.KeyedStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.Collector;

/**
 * 使用流的方式进行单词统计
 * 读取txt文件里面的数据，对单词进行空格拆分，对每个单词进行计算，然后进行分组（分流）累加
 *
 * 流作业与批作业的区别：
 * 1：批作业需要数据事先存在，流作业数据事先可以不存在
 * 2：批作业数据是有界的，流作业的数据是无界
 * 3：批处理作业运行完成作业停止，流作业不会主动停止
 */
public class StreamWordCount {
    public static void main(String[] args) throws Exception {
        /**
         * 实现步骤：
         * 1）初始化flink流处理的运行环境
         * 2）指定文件数据源，获取数据
         * 3）对获取到的数据进行空格拆分
         * 4）对拆分的单词进行计数，每个单词记一次数
         * 5）对相同的单词进行分组操作
         * 6）对分组后的数据进行累加操作
         * 7）打印输出（测试）
         * 8）启动作业，递交任务
         */
        // 初始化flink流处理的运行环境
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 设置并行度
        env.setParallelism(2);

        // 指定文件数据源，获取数据
        final String filePath = "C:\\Users\\13719\\Documents\\game-x-m\\src\\main\\java\\org\\smartloli\\game\\x\\m\\flink\\test.txt"; // 修改为本地txt文件路径
        final DataStreamSource<String> fileTextStream = env.readTextFile(filePath);

        // 对获取到的数据进行空格拆分
        final SingleOutputStreamOperator<String> words = fileTextStream.flatMap(
                new FlatMapFunction<String, String>() {
                    @Override
                    public void flatMap(String line, Collector<String> out) throws Exception {
                        // 将每行字符串进行空格拆分
                        final String[] dataArray = line.split(" ");
                        // 循环遍历字符串数组
                        for (String word : dataArray) {
                            // 需要使用out进行返回数据
                            out.collect(word);
                        }
                    }
                });

        // 对拆分的单词进行计数，每个单词记一次数
        final SingleOutputStreamOperator<Tuple2<String, Integer>> wordAndOne = words.map(
                new MapFunction<String, Tuple2<String, Integer>>() {
                    @Override
                    public Tuple2<String, Integer> map(String value) throws Exception {
                        return Tuple2.of(value, 1);
                    }
                });

        // 对相同的单词进行分组操作
        final KeyedStream<Tuple2<String, Integer>, Tuple> keyedStream = wordAndOne.keyBy(0);

        // 对分组后的数据进行累加操作
        final SingleOutputStreamOperator<Tuple2<String, Integer>> summed = keyedStream.sum(1);

        // 打印输出（测试）
        summed.print();

        // 启动作业，递交任务
        env.execute();
    }
}

```

输出结果：

```
1> (hello,1)
2> (abc,1)
2> (world,1)
2> (flink,1)
1> (hello,2)
1> (hello,3)
1> (hello,4)
2> (flink,2)
```

#### 有状态、无状态

##### 有状态

当前数据流在处理时，算子的状态随着输入数据变化而变化，并将状态保存至内存/外存/hdfs。例：可以在流处理过程中维护一个计数器来记录处理过的数据条数，或者保存某些中间计算的结果

应用场景：在线机器学习、累计统计用户的点击量、交易量等

##### 无状态

每条数据的处理是相互独立的，不依赖于之前的数据或者状态信息。例：过滤操作，每条记录的处理仅仅取决于当前记录本身

应用场景：简单过滤操作、实时监控和警报

#### 窗口、WaterMark

##### 窗口

将无界的流划分为有限的片段

时间窗口、计数窗口、会话窗口

##### WaterMark

与event time或processing time比较，触发窗口计算

类比kafka中队列的offset

#### 算子状态、键控状态

算子状态用于管理全局状态

例：Kafka Source需要存储每个分区的偏移量，以记录每个任务已消费的数据位置

键控状态用于管理局部状态，更高效，因为可以使用如hash tabel直接定位到目标状态

例：对某个股票交易的数据进行一分钟的滚动窗口内最大值计算，可以对交易数据用股票代码进行键控



#### Flink作业调度机制

1.**作业提交**：用户通过Flink提交作业，作业被解析并转换成JobGraph

2.**JobManager接受作业**：JobGraph被提交到JobManager，JobManager将其转换成ExecutionGraph

3.**资源分配**：JobManager向ResourceManager请求TaskManager，确保有足够资源来执行作业

4.**任务分配**：在资源充足情况下，JobManager将ExecutionGraph切分成多个并行子任务，并将这些任务分配给TaskManager

5.**任务执行**：TaskManager接收到任务后，执行相应的计算，并将执行结果发送回JobManager进行监控和调度